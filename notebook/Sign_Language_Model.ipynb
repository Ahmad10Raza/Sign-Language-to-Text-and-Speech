{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importing the OS module for directory handling\n",
    "import cv2  # Importing OpenCV for image capturing\n",
    "\n",
    "# Define the directory where the dataset will be stored\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):  # Check if the directory exists\n",
    "    os.makedirs(DATA_DIR)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the number of classes (different sign language gestures) and dataset size per class\n",
    "number_of_classes = 38  # Total number of different signs to be captured\n",
    "dataset_size = 100  # Number of images per sign\n",
    "\n",
    "# Open the webcam for capturing images\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop through each class to collect images\n",
    "for j in range(number_of_classes):\n",
    "    # Create a directory for the current class if it doesn't exist\n",
    "    class_dir = os.path.join(DATA_DIR, str(j))\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "\n",
    "    print(f'Collecting data for class {j}')\n",
    "\n",
    "    # Wait for user input before starting to collect data\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Capture a frame from the webcam\n",
    "        if not ret:\n",
    "            break  # If no frame is captured, exit loop\n",
    "\n",
    "        # Display a message on the frame to inform the user\n",
    "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)  # Show the frame\n",
    "        \n",
    "        # Wait for the user to press 'q' to start capturing images\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "\n",
    "    counter = 0  # Initialize image counter\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()  # Capture a frame\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('frame', frame)  # Display the frame\n",
    "        cv2.waitKey(25)  # Wait briefly before capturing the next frame\n",
    "        \n",
    "        # Save the captured frame as an image in the respective class folder\n",
    "        image_path = os.path.join(class_dir, f'{counter}.jpg')\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        \n",
    "        counter += 1  # Increment the counter\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code successfully captures and stores images for each sign class using OpenCV. Now that you have your dataset, what’s our next step? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Initialize Mediapipe Hands solution\n",
    "mp_hands = mp.solutions.hands  # Load Mediapipe Hands module\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)  # Set up hand detection model\n",
    "\n",
    "# Define dataset directory and expected number of features per sample\n",
    "DATA_DIR = './data'  # Directory where dataset images are stored\n",
    "EXPECTED_FEATURES = 42  # Expected number of features (21 hand landmarks × 2 coordinates)\n",
    "\n",
    "# Lists to store processed data and corresponding labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each class directory in the dataset\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []  # Temporary list to store landmark features for one image\n",
    "        x_ = []  # List to store x-coordinates of landmarks\n",
    "        y_ = []  # List to store y-coordinates of landmarks\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))  # Load the image\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB (required by Mediapipe)\n",
    "\n",
    "        # Process the image using Mediapipe Hands to extract hand landmarks\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:  # Check if any hands were detected\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x  # Extract x-coordinate\n",
    "                    y = hand_landmarks.landmark[i].y  # Extract y-coordinate\n",
    "\n",
    "                    x_.append(x)  # Store x-coordinate\n",
    "                    y_.append(y)  # Store y-coordinate\n",
    "\n",
    "                # Normalize landmark coordinates by subtracting the minimum x and y values\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))  # Store normalized x-coordinate\n",
    "                    data_aux.append(y - min(y_))  # Store normalized y-coordinate\n",
    "\n",
    "            # Ensure the extracted data matches the expected feature count before adding to dataset\n",
    "            if len(data_aux) == EXPECTED_FEATURES:\n",
    "                data.append(data_aux)  # Add processed data\n",
    "                labels.append(dir_)  # Add corresponding label\n",
    "            else:\n",
    "                print(f\"Skipped image {img_path} in {dir_}: incomplete data with {len(data_aux)} features.\")\n",
    "\n",
    "# Save the dataset as a pickle file for later use\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump({'data': data, 'labels': labels}, f)\n",
    "\n",
    "print(f\"Dataset saved. Total samples: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes your collected sign language images and extracts hand landmark features using Mediapipe. Here’s a breakdown of what it does:\n",
    "\n",
    "1. **Initialize Mediapipe Hands**:  \n",
    "   - Loads the Mediapipe Hands module to detect and extract hand landmarks.\n",
    "   - Sets it to process static images with a minimum confidence of 0.3.\n",
    "\n",
    "2. **Iterate through the dataset directory**:  \n",
    "   - Loops through each class (gesture) folder and its images.\n",
    "\n",
    "3. **Process each image**:  \n",
    "   - Reads the image and converts it to RGB.\n",
    "   - Uses Mediapipe to detect hand landmarks.\n",
    "\n",
    "4. **Extract and normalize hand landmarks**:  \n",
    "   - Stores x and y coordinates of 21 hand landmarks.\n",
    "   - Normalizes them by subtracting the minimum x and y values.\n",
    "\n",
    "5. **Filter valid data**:  \n",
    "   - Ensures the extracted features match the expected 42 (21 landmarks × 2).\n",
    "   - If valid, appends the data and its corresponding class label.\n",
    "\n",
    "6. **Save the processed dataset**:  \n",
    "   - Stores the extracted hand landmark data and labels in a pickle file (`data.pickle`) for later model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Pickle datasets?\n",
    "\n",
    "Using a pickle file is a common practice for the following reasons:\n",
    "\n",
    "1. **Efficiency in Loading Processed Data**:  \n",
    "   Once you extract the hand landmark features from your images, saving them as a pickle file allows you to quickly reload this structured data without having to re-run the computationally expensive image processing step every time you train or test your model.\n",
    "\n",
    "2. **Structured Data Format**:  \n",
    "   The processed data consists of numerical feature vectors (e.g., normalized landmark coordinates) that are much more manageable for machine learning models compared to raw image data. Models like Random Forests or other classical ML algorithms work best with these structured numerical arrays.\n",
    "\n",
    "3. **Storage and Reusability**:  \n",
    "   Pickle files can store complex Python objects (like dictionaries, lists, numpy arrays, etc.) in a serialized binary format. This makes it convenient to save and share your preprocessed dataset across different parts of your project or even with other collaborators.\n",
    "\n",
    "4. **Separation of Concerns**:  \n",
    "   By converting images into feature vectors and storing them separately, you separate the heavy-lifting of image processing from model training. This modular approach allows you to experiment with different machine learning models without having to repeatedly process raw images.\n",
    "\n",
    "While you can use images directly, that approach is more common with deep learning models (e.g., convolutional neural networks) that learn features from raw pixel data. In your case, since you've already extracted and normalized hand landmarks, using a pickle file to store this data is both efficient and practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Mediapipe Hands solution\n",
    "mp_hands = mp.solutions.hands  # Load Mediapipe Hands module\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)  # Set up hand detection model\n",
    "\n",
    "# Define dataset directory and expected number of features per sample\n",
    "DATA_DIR = './data'  # Directory where dataset images are stored\n",
    "EXPECTED_FEATURES = 42  # Expected number of features (21 hand landmarks × 2 coordinates)\n",
    "\n",
    "# Lists to store processed data and corresponding labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each class directory in the dataset\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []  # Temporary list to store landmark features for one image\n",
    "        x_ = []  # List to store x-coordinates of landmarks\n",
    "        y_ = []  # List to store y-coordinates of landmarks\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))  # Load the image\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB (required by Mediapipe)\n",
    "\n",
    "        # Process the image using Mediapipe Hands to extract hand landmarks\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:  # Check if any hands were detected\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x  # Extract x-coordinate\n",
    "                    y = hand_landmarks.landmark[i].y  # Extract y-coordinate\n",
    "\n",
    "                    x_.append(x)  # Store x-coordinate\n",
    "                    y_.append(y)  # Store y-coordinate\n",
    "\n",
    "                # Normalize landmark coordinates by subtracting the minimum x and y values\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))  # Store normalized x-coordinate\n",
    "                    data_aux.append(y - min(y_))  # Store normalized y-coordinate\n",
    "\n",
    "            # Ensure the extracted data matches the expected feature count before adding to dataset\n",
    "            if len(data_aux) == EXPECTED_FEATURES:\n",
    "                data.append(data_aux)  # Add processed data\n",
    "                labels.append(dir_)  # Add corresponding label\n",
    "            else:\n",
    "                print(f\"Skipped image {img_path} in {dir_}: incomplete data with {len(data_aux)} features.\")\n",
    "\n",
    "# Save the dataset as a pickle file for later use\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump({'data': data, 'labels': labels}, f)\n",
    "\n",
    "print(f\"Dataset saved. Total samples: {len(data)}\")\n",
    "\n",
    "# Load the dataset from the pickle file\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "data = np.asarray(data_dict['data'])  # Convert data to numpy array\n",
    "labels = np.asarray(data_dict['labels'])  # Convert labels to numpy array\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "# Initialize the machine learning model (Random Forest Classifier)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "# Print the accuracy result\n",
    "print('{}% of samples were classified correctly!'.format(score * 100))\n",
    "\n",
    "# Save the trained model as a pickle file for later use\n",
    "with open('model.p', 'wb') as f:\n",
    "    pickle.dump({'model': model}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
